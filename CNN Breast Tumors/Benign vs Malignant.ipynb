{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aedd5f3-c4e3-4fca-ba55-02f717b8d363",
   "metadata": {},
   "source": [
    "# Use A CNN To Process The Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac1905-7938-4fe2-bf11-af3d21ef16f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Requirements \n",
    "\n",
    "### Datasets:\n",
    "Kaggle Data set\n",
    "The BreaKHis database contains microscopic biopsy images benign and malignant breast tumors, wherein this dataset, I have done to separate training data and test data with different folders, each file has a different slide image. In this dataset I only took a partial sample of 400x optical zoom, if you are further interested in the dataset, please refer to this paper:\n",
    "\n",
    "FA Spanhol, LS Oliveira, C. Petitjean and L. Heutte, \"A Dataset for Breast Cancer Histopathological Image Classification,\" in IEEE Transactions on Biomedical Engineering, vol. 63, no. 7, pp. 1455-1462, July 2016, doi: 10.1109 / TBME.2015.2496264.\n",
    "\n",
    "Given this, the dataset is not in my control. This dataset purpose is to deep learning learner\n",
    "\n",
    "### Python:\n",
    "Python3 (3.9.18)\n",
    "\n",
    "### Modules:\n",
    "TensorFlow (2.14.0)\n",
    "\n",
    "Numpy (1.24.3)\n",
    "\n",
    "Pillow (10.0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5543e1-28b2-4053-b732-9099e997bc44",
   "metadata": {},
   "source": [
    "The Goal of this will be to build a CNN to determine malignant vs benign tumors based on patient slides. \n",
    "\n",
    "To Do List\n",
    "1) Load the data and Assess the data imaging slides to get the amount of slides as well as how best to proceed\n",
    "Two options will be pursued, both a down sampling and and up sampling with affine \n",
    "2a) down sample to assess items\n",
    "2b) run augmentations and other transformations as this will enable us get some practice in\n",
    "one question i have is what is the difference betweem skimage and keras preprocessing\n",
    "It seems that unless we are running real imaging analysis such as pixel based or edge detection or segemention that its better to run it through keras because it is more efficient at setting up the augmentation transformations. \n",
    "\n",
    "Thus I will use keras data preprocessing image and feed it through the keras tensforflow cnn\n",
    "\n",
    "\n",
    "3) setup these slides so that we have an equal amount of images in both the malignant and the benign set in the training set\n",
    "4) setup the cnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39f432-f015-4234-8998-65460ee1f5cf",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e167b9f-9b63-447b-93e4-6c186c961d90",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23d9a63d-9f1e-4cc7-ad68-cd8ee1b4692a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "87e8a65c-3607-43fd-959d-f6dce90444d1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc46cb-853f-48ef-a722-987d6d17a9e1",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Loading And PreProcessing The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5551f1c3-5fdc-43b3-8bfb-859d12af9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply transformation to training set only. Applying only to the training set avoids overfitting.\n",
    "#Transformations are geometric transformations, rotate images, zoom in/out, flip images, etc. This is called augmentation\n",
    "#Augmentation avoids overtraining on the training set because it augments the variety of original images to avoid overfit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dabc5dbf-1061-4af2-8aa2-14fe298936dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = 'P:/Portfolio Sets/Benign vs Malignant Slides Classification/BreaKHis 400X/train' #watch the direction of slashes, '\\' will confuse python use '/' or '\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b403a6-4e64-4d96-8770-c2b9528a4ebe",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### The Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44fc8d8e-685c-4cd7-a8a2-2656a89935c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1148 files belonging to 2 classes.\n",
      "Using 919 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path, #file path\n",
    "    labels = 'inferred',  # Automatically infer labels from directory structure (folder names)\n",
    "    label_mode = 'categorical',  # categorical/int/etc.\n",
    "    image_size = (64, 64), # Target image size for resizing\n",
    "    batch_size = 32,  # Batch size for training\n",
    "    seed = 100, #same random selection instance each time, required for splitting into training and validation sets\n",
    "    validation_split = 0.2,  # Split the dataset into training and validation sets\n",
    "    subset = 'training')  # Specify if it's the training subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f7f773f-68f9-4b55-9814-0d1c03b19555",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['benign', 'malignant']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the keras dataset from directory is unable to detect secondary sub folders thus if I select a folder the next step will determine the layers \n",
    "# sub folders past the first layer in otherwords I need to ensure to select\n",
    "train_dataset.class_names # selects what the label set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b2e9b3b3-6116-45b4-93f3-89aef8386947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1148 files belonging to 2 classes.\n",
      "Using 229 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['benign', 'malignant']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation set\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path, \n",
    "    labels = 'inferred',\n",
    "    label_mode ='categorical',\n",
    "    image_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    seed = 100,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation')  # Specify if it's the validation subset\n",
    "\n",
    "validation_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3d798-f49d-4d93-b4d2-c5f8c889371b",
   "metadata": {
    "colab_type": "text",
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118018fc-7210-401b-a6c5-2b8ae9f08e99",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b50dfb24-d9b3-462c-b893-dc06238aee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c5dc0d0-1496-43f3-af43-af530e82098b",
   "metadata": {
    "colab_type": "text",
    "id": "ces1gXY2lmoX"
   },
   "outputs": [],
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c48ac-7dcd-41e1-a04b-1434219e5c65",
   "metadata": {
    "colab_type": "text",
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fea0dccf-f64a-45cd-8c14-9df2b6ef3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Layer\n",
    "cnn.add(tf.keras.layers.Conv2D( #add function applies new layer\n",
    "    filters = 32, # Number of features\n",
    "    kernel_size = 3, # Dimensions of feature detector (single digit is squared (X -> 3 x 3) or paired acceptable (X, Y -> (X x Y)  \n",
    "    activation = 'relu', # Activation type \n",
    "    input_shape = (64, 64, 3))) # Tuple that selects image properties (batch size(optional), size, size, 3(RGB) or 1(B&W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c68bb1-810c-4f6c-b9a9-ca0a1fef1d32",
   "metadata": {
    "colab_type": "text",
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e8943ce-c130-4caf-9138-f65c19a33071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling - Down-Sampling Operation that ???????\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size = 2, # frame size of the pool (single digit is squared (X -> 3 x 3) or paired acceptable (X, Y -> (X x Y)  \n",
    "    strides = 2, # pixels the frame will move over when pooling (single digit is squared (X -> 3 x 3) or paired acceptable (X, Y -> (X x Y)\n",
    "    padding = 'valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb32e7-9daf-4bd4-886d-6ba42a75aa8b",
   "metadata": {
    "colab_type": "text",
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbab1c-90c0-4c0f-8428-4efaa4786857",
   "metadata": {},
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation='relu'))\n",
    "# input layer was removed because it was already applied earlier in the first convolutional layer\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2,padding='valid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc65f8-24e7-471e-9ccc-e0cf8182465d",
   "metadata": {
    "colab_type": "text",
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "841bdeb8-c866-44e3-88e4-52f2ad2714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening - Does what\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "#automatically flattens all the CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163ac3e-2e3b-4ec7-b753-634a66c9d01b",
   "metadata": {
    "colab_type": "text",
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4208cc69-2383-4dc8-ad7b-28fbdab489a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting - Connect the Layers\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "# units = the number of neurons for this layer (higher usually means more accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaf7dc-8f5a-42a7-84b8-4025d4c5751a",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "357a4c4d-2678-43c1-a78f-2f0607dc291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Layer - Final Layer To Predict Classification\n",
    "cnn.add(tf.keras.layers.Dense(units = 2, activation='softmax'))\n",
    "# set the number of neurons for final classification output, binary (units = 1) vs for multiclass/categorical (units = number of categories)\n",
    "#activation will be sigmoid for binary (units = 1), for multiclass(categorical) could softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d680f9-8de3-466b-95ec-c81bdde53675",
   "metadata": {},
   "source": [
    "### Step 6 - Compile The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6d03b81-600e-4d29-8ea4-b66c13886d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "#compiles the cnn to the optimizer and loss function using accuracy as the metric "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdc0c5-9abb-46e3-9a12-77346be8c5bd",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl",
    "tags": []
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26790505-dbe8-4173-a125-28aff72107f2",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "359ed537-4e50-4d8e-bd9e-67c583e5c31b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "29/29 [==============================] - 6s 151ms/step - loss: 218.2420 - accuracy: 0.5985 - val_loss: 15.8445 - val_accuracy: 0.4541\n",
      "Epoch 2/25\n",
      "29/29 [==============================] - 5s 143ms/step - loss: 6.3123 - accuracy: 0.7127 - val_loss: 2.0074 - val_accuracy: 0.8035\n",
      "Epoch 3/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 9.9402 - accuracy: 0.6866 - val_loss: 2.3109 - val_accuracy: 0.8559\n",
      "Epoch 4/25\n",
      "29/29 [==============================] - 5s 146ms/step - loss: 2.1781 - accuracy: 0.8096 - val_loss: 1.5825 - val_accuracy: 0.8297\n",
      "Epoch 5/25\n",
      "29/29 [==============================] - 5s 145ms/step - loss: 1.1501 - accuracy: 0.8226 - val_loss: 3.6361 - val_accuracy: 0.7380\n",
      "Epoch 6/25\n",
      "29/29 [==============================] - 5s 144ms/step - loss: 0.5284 - accuracy: 0.9010 - val_loss: 5.6546 - val_accuracy: 0.6987\n",
      "Epoch 7/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 2.2897 - accuracy: 0.7922 - val_loss: 1.2394 - val_accuracy: 0.8690\n",
      "Epoch 8/25\n",
      "29/29 [==============================] - 5s 147ms/step - loss: 1.0027 - accuracy: 0.8977 - val_loss: 0.9978 - val_accuracy: 0.8603\n",
      "Epoch 9/25\n",
      "29/29 [==============================] - 5s 145ms/step - loss: 0.1215 - accuracy: 0.9619 - val_loss: 1.1112 - val_accuracy: 0.8166\n",
      "Epoch 10/25\n",
      "29/29 [==============================] - 5s 146ms/step - loss: 0.0936 - accuracy: 0.9739 - val_loss: 1.0635 - val_accuracy: 0.8428\n",
      "Epoch 11/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 0.0380 - accuracy: 0.9859 - val_loss: 0.9212 - val_accuracy: 0.8646\n",
      "Epoch 12/25\n",
      "29/29 [==============================] - 5s 145ms/step - loss: 0.0369 - accuracy: 0.9891 - val_loss: 1.1663 - val_accuracy: 0.8341\n",
      "Epoch 13/25\n",
      "29/29 [==============================] - 5s 147ms/step - loss: 0.1437 - accuracy: 0.9521 - val_loss: 1.4729 - val_accuracy: 0.8297\n",
      "Epoch 14/25\n",
      "29/29 [==============================] - 5s 146ms/step - loss: 0.0543 - accuracy: 0.9826 - val_loss: 1.1421 - val_accuracy: 0.8297\n",
      "Epoch 15/25\n",
      "29/29 [==============================] - 5s 146ms/step - loss: 0.0157 - accuracy: 0.9956 - val_loss: 0.9879 - val_accuracy: 0.8603\n",
      "Epoch 16/25\n",
      "29/29 [==============================] - 5s 146ms/step - loss: 0.4687 - accuracy: 0.9608 - val_loss: 1.2797 - val_accuracy: 0.8472\n",
      "Epoch 17/25\n",
      "29/29 [==============================] - 5s 150ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 1.0611 - val_accuracy: 0.8603\n",
      "Epoch 18/25\n",
      "29/29 [==============================] - 5s 150ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.1291 - val_accuracy: 0.8515\n",
      "Epoch 19/25\n",
      "29/29 [==============================] - 5s 151ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 1.1388 - val_accuracy: 0.8515\n",
      "Epoch 20/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0992 - val_accuracy: 0.8559\n",
      "Epoch 21/25\n",
      "29/29 [==============================] - 5s 156ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.1412 - val_accuracy: 0.8559\n",
      "Epoch 22/25\n",
      "29/29 [==============================] - 5s 152ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.1768 - val_accuracy: 0.8559\n",
      "Epoch 23/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.1822 - val_accuracy: 0.8559\n",
      "Epoch 24/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2078 - val_accuracy: 0.8559\n",
      "Epoch 25/25\n",
      "29/29 [==============================] - 5s 150ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1515 - val_accuracy: 0.8603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27261802130>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_dataset, validation_data = validation_dataset, epochs = 25)\n",
    "#trains the cnn\n",
    "#need to look closer into model.fit() parameters and why we use validation_data and not a y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25cf36de-1970-4479-8cbb-6a31975f3e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "29/29 [==============================] - 6s 158ms/step - loss: 138.7313 - accuracy: 0.5822 - val_loss: 1.3720 - val_accuracy: 0.7904\n",
      "Epoch 2/25\n",
      "29/29 [==============================] - 5s 154ms/step - loss: 1.5355 - accuracy: 0.7193 - val_loss: 3.3650 - val_accuracy: 0.7031\n",
      "Epoch 3/25\n",
      "29/29 [==============================] - 5s 145ms/step - loss: 1.7055 - accuracy: 0.7987 - val_loss: 0.8639 - val_accuracy: 0.6594\n",
      "Epoch 4/25\n",
      "29/29 [==============================] - 5s 143ms/step - loss: 0.4207 - accuracy: 0.8422 - val_loss: 0.8188 - val_accuracy: 0.7336\n",
      "Epoch 5/25\n",
      "29/29 [==============================] - 5s 147ms/step - loss: 0.8443 - accuracy: 0.8150 - val_loss: 0.6951 - val_accuracy: 0.8865\n",
      "Epoch 6/25\n",
      "29/29 [==============================] - 5s 155ms/step - loss: 2.2707 - accuracy: 0.7726 - val_loss: 1.6462 - val_accuracy: 0.8253\n",
      "Epoch 7/25\n",
      "29/29 [==============================] - 5s 147ms/step - loss: 0.5734 - accuracy: 0.8727 - val_loss: 1.0365 - val_accuracy: 0.7729\n",
      "Epoch 8/25\n",
      "29/29 [==============================] - 5s 145ms/step - loss: 0.1908 - accuracy: 0.9358 - val_loss: 0.5018 - val_accuracy: 0.8865\n",
      "Epoch 9/25\n",
      "29/29 [==============================] - 5s 153ms/step - loss: 0.0579 - accuracy: 0.9793 - val_loss: 0.4419 - val_accuracy: 0.8777\n",
      "Epoch 10/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.9639 - accuracy: 0.8879 - val_loss: 1.4355 - val_accuracy: 0.8515\n",
      "Epoch 11/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.6322 - accuracy: 0.8651 - val_loss: 0.8563 - val_accuracy: 0.7729\n",
      "Epoch 12/25\n",
      "29/29 [==============================] - 5s 151ms/step - loss: 0.1805 - accuracy: 0.9325 - val_loss: 1.0402 - val_accuracy: 0.8253\n",
      "Epoch 13/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.2522 - accuracy: 0.9206 - val_loss: 1.0822 - val_accuracy: 0.8384\n",
      "Epoch 14/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.7962 - accuracy: 0.9238 - val_loss: 3.3312 - val_accuracy: 0.4585\n",
      "Epoch 15/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 0.3687 - accuracy: 0.9021 - val_loss: 0.8449 - val_accuracy: 0.8559\n",
      "Epoch 16/25\n",
      "29/29 [==============================] - 5s 147ms/step - loss: 0.0422 - accuracy: 0.9848 - val_loss: 0.6679 - val_accuracy: 0.8646\n",
      "Epoch 17/25\n",
      "29/29 [==============================] - 5s 148ms/step - loss: 0.0230 - accuracy: 0.9935 - val_loss: 0.6866 - val_accuracy: 0.8690\n",
      "Epoch 18/25\n",
      "29/29 [==============================] - 5s 150ms/step - loss: 0.0185 - accuracy: 0.9967 - val_loss: 0.7064 - val_accuracy: 0.8646\n",
      "Epoch 19/25\n",
      "29/29 [==============================] - 5s 155ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.7195 - val_accuracy: 0.8690\n",
      "Epoch 20/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.0136 - accuracy: 0.9978 - val_loss: 0.7261 - val_accuracy: 0.8690\n",
      "Epoch 21/25\n",
      "29/29 [==============================] - 5s 150ms/step - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.7410 - val_accuracy: 0.8734\n",
      "Epoch 22/25\n",
      "29/29 [==============================] - 5s 152ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.7509 - val_accuracy: 0.8690\n",
      "Epoch 23/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.7656 - val_accuracy: 0.8734\n",
      "Epoch 24/25\n",
      "29/29 [==============================] - 5s 149ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.7850 - val_accuracy: 0.8690\n",
      "Epoch 25/25\n",
      "29/29 [==============================] - 5s 151ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8028 - val_accuracy: 0.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ec8bf73c70>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)), #Conv2d(filter (# of features) = 32, kernel_size = (size of  feature detector a 3 is 3x3), , activation = 'relu' for rectifier activation, input_shape = [size, size, rgb(3) or bw(1)] \n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2, activation = 'softmax')  # This is a multilabel categorical classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train your model\n",
    "model.fit(train_dataset, validation_data = validation_dataset, epochs = 25)  # Adjust the number of epochs as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69deaf6b-6fc5-49a0-bf29-8debb54d818f",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7b8f1-9a9c-49e7-82bd-52920f87f2d0",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras import utils\n",
    "test_image = utils.load_img('P:\\Machine-Learning Course\\Machine Learning A-Z (Codes and Datasets)\\AG Worksheets\\dataset\\Single Prediction\\dogorcat1.jpg', target_size = (64, 64))\n",
    "test_image = utils.img_to_array(test_image) #changes to an array to the cnn model can analyze\n",
    "test_image = np.expand_dims(test_image, axis = 0) #adds an extra dimension to enable the image to have the batch dimension\n",
    "#batch dimension required because the model has to run at a certain batch number, in this example 32\n",
    "#dimension of batch is added to the 1st dimension\n",
    "\n",
    "#we can now run the predict method\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "#to figure out which class is index as what\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1: #result 1st slot as batch dimension so we run result[0] to enter the batch the next [0] selects the element in the batch(the single dog image) \n",
    "    prediction = 'Dog'\n",
    "    \n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004a3a3-b68e-4a83-a090-46d3e3795dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
