{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aedd5f3-c4e3-4fca-ba55-02f717b8d363",
   "metadata": {},
   "source": [
    "# Use A CNN To Process The Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac1905-7938-4fe2-bf11-af3d21ef16f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Requirements \n",
    "\n",
    "### Datasets:\n",
    "Kaggle Data Set:\n",
    "\n",
    "### Python:\n",
    "Python3 (3.9.18)\n",
    "\n",
    "### Modules:\n",
    "TensorFlow (2.14.0)\n",
    "\n",
    "Numpy (1.24.3)\n",
    "\n",
    "Pillow (10.0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5543e1-28b2-4053-b732-9099e997bc44",
   "metadata": {},
   "source": [
    "The Goal of this will be to build a CNN to determine malignant vs benign tumors based on patient slides. \n",
    "\n",
    "To Do List\n",
    "1) Load the data and Assess the data imaging slides to get the amount of slides as well as how best to proceed\n",
    "Two options will be pursued, both a down sampling and and up sampling with affine \n",
    "2a) down sample to assess items\n",
    "2b) run augmentations and other transformations as this will enable us get some practice in\n",
    "one question i have is what is the difference betweem skimage and keras preprocessing\n",
    "It seems that unless we are running real imaging analysis such as pixel based or edge detection or segemention that its better to run it through keras because it is more efficient at setting up the augmentation transformations. \n",
    "\n",
    "Thus I will use keras data preprocessing image and feed it through the keras tensforflow cnn\n",
    "\n",
    "\n",
    "3) setup these slides so that we have an equal amount of images in both the malignant and the benign set in the training set\n",
    "4) setup the cnn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39f432-f015-4234-8998-65460ee1f5cf",
   "metadata": {
    "colab_type": "text",
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e167b9f-9b63-447b-93e4-6c186c961d90",
   "metadata": {
    "colab_type": "text",
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d9a63d-9f1e-4cc7-ad68-cd8ee1b4692a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCV30xyVhFbE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e8a65c-3607-43fd-959d-f6dce90444d1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FIleuCAjoFD8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.13\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc46cb-853f-48ef-a722-987d6d17a9e1",
   "metadata": {
    "colab_type": "text",
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabc5dbf-1061-4af2-8aa2-14fe298936dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = 'P:/Portfolio Sets/CNN Breast Tumors/BreaKHis 400X/train' #watch the direction of slashes, '\\' will confuse python use '/' or '\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b403a6-4e64-4d96-8770-c2b9528a4ebe",
   "metadata": {
    "colab_type": "text",
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### The Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44fc8d8e-685c-4cd7-a8a2-2656a89935c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1148 files belonging to 2 classes.\n",
      "Using 919 files for training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['benign', 'malignant']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path, #file path\n",
    "    labels = 'inferred',  # Automatically infer labels from directory structure (folder names)\n",
    "    label_mode = 'categorical',  # categorical/int/etc.\n",
    "    color_mode='rgb', #sets the color of the images rgb/grayscale\n",
    "    image_size = (64, 64), # Target image size for resizing\n",
    "    batch_size = 32,  # Batch size for training\n",
    "    seed = 100, #same random selection instance each time, required for splitting into training and validation sets\n",
    "    validation_split = 0.2,  # Split the dataset into training and validation sets\n",
    "    subset = 'training')  # Specify if it's the training subset\n",
    "\n",
    "train_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908430f-f0c2-4c38-a6ad-6cd6ba76f7e2",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e9b3b3-6116-45b4-93f3-89aef8386947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1148 files belonging to 2 classes.\n",
      "Using 229 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['benign', 'malignant']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation set\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path, \n",
    "    labels = 'inferred',\n",
    "    label_mode ='categorical',\n",
    "    color_mode='rgb',\n",
    "    image_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    seed = 100,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation', # Specify if it's the validation subset)  \n",
    ")\n",
    "validation_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82763206-eb8b-4eb5-8258-10af1034a76f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2 - Preprocessing and Augmentation \n",
    "Augmentation avoids overfitting the training data.\n",
    "Apply Augmentation to training set only. Applying only to the training set avoids overfitting.\n",
    "Transformations are geometric transformations, rotate images, zoom in/out, flip images, etc. This is called augmentation\n",
    "Augmentation can be applied to the dataset at 1 of 2 different points\n",
    "\n",
    "Option 1) The Preprocessing Step prior to CNN model (Easier Modularity and Maintenance)\n",
    "\n",
    "Option 2) Included in the actual CNN model (Easier Deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118018fc-7210-401b-a6c5-2b8ae9f08e99",
   "metadata": {
    "colab_type": "text",
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 3 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eaf7dc-8f5a-42a7-84b8-4025d4c5751a",
   "metadata": {
    "colab_type": "text",
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Convolution and Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4abc4757-05d5-4fb7-9ac7-3c7041b91fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation ='relu', input_shape = (64, 64, 3)),\n",
    "    tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2, padding = 'valid'),\n",
    "    tf.keras.layers.Flatten(), #last layer before moving data to neural network\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d680f9-8de3-466b-95ec-c81bdde53675",
   "metadata": {},
   "source": [
    "### Compile The CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d03b81-600e-4d29-8ea4-b66c13886d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cdc0c5-9abb-46e3-9a12-77346be8c5bd",
   "metadata": {
    "colab_type": "text",
    "id": "D6XkI90snSDl",
    "tags": []
   },
   "source": [
    "## Part 4 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26790505-dbe8-4173-a125-28aff72107f2",
   "metadata": {
    "colab_type": "text",
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359ed537-4e50-4d8e-bd9e-67c583e5c31b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "29/29 [==============================] - 17s 417ms/step - loss: 179.4126 - accuracy: 0.6605 - val_loss: 2.8058 - val_accuracy: 0.6070\n",
      "Epoch 2/15\n",
      "29/29 [==============================] - 3s 97ms/step - loss: 3.6611 - accuracy: 0.6877 - val_loss: 1.5539 - val_accuracy: 0.6900\n",
      "Epoch 3/15\n",
      "29/29 [==============================] - 3s 100ms/step - loss: 2.9460 - accuracy: 0.7476 - val_loss: 2.0600 - val_accuracy: 0.8297\n",
      "Epoch 4/15\n",
      "29/29 [==============================] - 3s 95ms/step - loss: 2.2461 - accuracy: 0.8074 - val_loss: 1.1065 - val_accuracy: 0.8821\n",
      "Epoch 5/15\n",
      "29/29 [==============================] - 3s 96ms/step - loss: 1.2627 - accuracy: 0.8716 - val_loss: 2.2109 - val_accuracy: 0.7336\n",
      "Epoch 6/15\n",
      "29/29 [==============================] - 3s 98ms/step - loss: 0.3000 - accuracy: 0.9206 - val_loss: 0.6353 - val_accuracy: 0.8384\n",
      "Epoch 7/15\n",
      "29/29 [==============================] - 3s 97ms/step - loss: 0.3403 - accuracy: 0.8803 - val_loss: 1.2536 - val_accuracy: 0.8166\n",
      "Epoch 8/15\n",
      "29/29 [==============================] - 3s 98ms/step - loss: 0.1706 - accuracy: 0.9576 - val_loss: 0.7617 - val_accuracy: 0.8646\n",
      "Epoch 9/15\n",
      "29/29 [==============================] - 3s 100ms/step - loss: 0.4121 - accuracy: 0.9195 - val_loss: 2.4543 - val_accuracy: 0.7555\n",
      "Epoch 10/15\n",
      "29/29 [==============================] - 3s 100ms/step - loss: 0.3127 - accuracy: 0.9380 - val_loss: 0.9939 - val_accuracy: 0.8690\n",
      "Epoch 11/15\n",
      "29/29 [==============================] - 3s 97ms/step - loss: 0.0508 - accuracy: 0.9815 - val_loss: 0.9010 - val_accuracy: 0.8253\n",
      "Epoch 12/15\n",
      "29/29 [==============================] - 3s 98ms/step - loss: 0.0934 - accuracy: 0.9695 - val_loss: 0.8610 - val_accuracy: 0.8559\n",
      "Epoch 13/15\n",
      "29/29 [==============================] - 3s 100ms/step - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.8647 - val_accuracy: 0.8515\n",
      "Epoch 14/15\n",
      "29/29 [==============================] - 3s 101ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8934 - val_accuracy: 0.8603\n",
      "Epoch 15/15\n",
      "29/29 [==============================] - 3s 101ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8931 - val_accuracy: 0.8603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f4fac69e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = train_dataset, validation_data = validation_dataset, epochs = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69deaf6b-6fc5-49a0-bf29-8debb54d818f",
   "metadata": {
    "colab_type": "text",
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7b8f1-9a9c-49e7-82bd-52920f87f2d0",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras import utils\n",
    "test_image = utils.load_img('P:\\Machine-Learning Course\\Machine Learning A-Z (Codes and Datasets)\\AG Worksheets\\dataset\\Single Prediction\\dogorcat1.jpg', target_size = (64, 64))\n",
    "test_image = utils.img_to_array(test_image) #changes to an array to the cnn model can analyze\n",
    "test_image = np.expand_dims(test_image, axis = 0) #adds an extra dimension to enable the image to have the batch dimension\n",
    "#batch dimension required because the model has to run at a certain batch number, in this example 32\n",
    "#dimension of batch is added to the 1st dimension\n",
    "\n",
    "#we can now run the predict method\n",
    "result = cnn.predict(test_image)\n",
    "\n",
    "#to figure out which class is index as what\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1: #result 1st slot as batch dimension so we run result[0] to enter the batch the next [0] selects the element in the batch(the single dog image) \n",
    "    prediction = 'Dog'\n",
    "    \n",
    "else:\n",
    "    prediction = 'Cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004a3a3-b68e-4a83-a090-46d3e3795dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
